{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8677f054-8045-48cc-a7d9-058ef9014278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import sys  \n",
    "import joblib\n",
    "sys.path.insert(0, './myCode')\n",
    "from PLModel import PLModel\n",
    "from utils import loadModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fc3f55-c084-4feb-887f-85966d0d1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Define image size per model (as you provided)\n",
    "imageSizePerCNN = {\n",
    "    \"Resnext50\": 600,\n",
    "    \"Resnet152\": 600,\n",
    "    \"EfficientNetB7\": 600,\n",
    "    \"EfficientNetB6\": 528,\n",
    "    \"EfficientNetB5\": 456,\n",
    "    \"EfficientNetB4\": 380,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28116974-9a6b-4dae-bbf0-c1ab8b0d8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final order in which we want to feed the (averaged) logits into the meta-classifier\n",
    "ARCHITECTURE_ORDER = [\n",
    "    \"EfficientNetB4\",\n",
    "    \"EfficientNetB5\",\n",
    "    \"EfficientNetB6\",\n",
    "    \"Resnext50\",\n",
    "    \"Resnet152\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0b64f288-9153-4c11-883f-1070429c3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class labels\n",
    "CLASS_LABELS = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]\n",
    "\n",
    "class SkinDiseaseClassifier:\n",
    "    \"\"\"\n",
    "    A class that loads multiple CNNs from a given folder structure and\n",
    "    runs inference on a skin disease image. Each model has its own image-size\n",
    "    requirement and possibly different architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_folder: str, \n",
    "        metaclassifier_name: str, \n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param base_folder: The path to the main folder containing subfolders for each model.\n",
    "                            Each subfolder should have a 'weights' directory with at least one .pth file.\n",
    "        :param device: The torch device to use ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.base_folder = base_folder\n",
    "\n",
    "        # Load the scikit-learn meta-classifier from 'metaclassifier' folder\n",
    "        self.meta_model = self._load_meta_classifier(metaclassifier_name)\n",
    "        # models_dict will map 'folder_name' -> {'model': model, 'transform': transform}\n",
    "        self.subfolder_models = self._load_all_models()\n",
    "\n",
    "    def _load_all_models(self):\n",
    "        \"\"\"\n",
    "        Iterates subfolders in `base_folder` and loads any that are not \"metaclassifier\".\n",
    "        Returns a dict:\n",
    "            {\n",
    "              \"Resnext50_0Fold\": ( \"Resnext50\", model, transform ),\n",
    "              \"Resnext50_1Fold\": ( \"Resnext50\", model, transform ),\n",
    "              \"EfficientNetB6_3Fold\": ( \"EfficientNetB6\", model, transform ),\n",
    "              ...\n",
    "            }\n",
    "        \"\"\"\n",
    "        subfolder_dict = {}\n",
    "        for entry in os.listdir(self.base_folder):\n",
    "            # skip the meta-classifier folder\n",
    "            if entry.lower() == \"metaclassifier\":\n",
    "                continue\n",
    "\n",
    "            model_subfolder = os.path.join(self.base_folder, entry)\n",
    "            if os.path.isdir(model_subfolder):\n",
    "                ckpt_files = [f for f in os.listdir(model_subfolder) if f.endswith('.ckpt')]\n",
    "                if not ckpt_files:\n",
    "                    print(f\"No .pth file found in {model_subfolder}, skipping.\")\n",
    "                    continue\n",
    "                weight_path = os.path.join(model_subfolder, ckpt_files[0])\n",
    "\n",
    "                # Identify architecture (e.g. \"Resnext50\") from folder name\n",
    "                arch_name = self._parse_base_model_name(entry)\n",
    "                # Build model + transform\n",
    "                model, transform = self._create_model_and_transform(arch_name, weight_path)\n",
    "\n",
    "                subfolder_dict[entry] = {\n",
    "                    \"name\" : arch_name, \n",
    "                    \"model\" : model, \n",
    "                    \"transform\" : transform\n",
    "                }\n",
    "        return subfolder_dict\n",
    "\n",
    "    def _parse_base_model_name(self, folder_name: str) -> str:\n",
    "        \"\"\"\n",
    "        E.g. \"Resnext50_0Fold\" -> \"Resnext50\".\n",
    "             \"EfficientNetB6_3Fold\" -> \"EfficientNetB6\".\n",
    "        We'll do a series of if/elif checks to match your naming.\n",
    "        \"\"\"\n",
    "        name_lower = folder_name.lower()\n",
    "        if 'efficientnetb4' in name_lower:\n",
    "            return \"EfficientNetB4\"\n",
    "        elif 'efficientnetb5' in name_lower:\n",
    "            return \"EfficientNetB5\"\n",
    "        elif 'efficientnetb6' in name_lower:\n",
    "            return \"EfficientNetB6\"\n",
    "        elif 'resnext' in name_lower:\n",
    "            return \"Resnext50\"\n",
    "        elif 'resnet152' in name_lower:\n",
    "            return \"Resnet152\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type in folder name: {folder_name}\")\n",
    "\n",
    "    def _load_meta_classifier(\n",
    "        self,\n",
    "        metaclassifier_name\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load the scikit-learn meta-classifier from the 'metaclassifier' subfolder.\n",
    "        For example, if you saved it as model.pkl or meta_model.pkl.\n",
    "        \"\"\"\n",
    "        # Adjust the filename as necessary:\n",
    "        meta_classifier_path = os.path.join('metaclassifiers', f'{metaclassifier_name}.pkl')\n",
    "        if not os.path.exists(meta_classifier_path):\n",
    "            raise FileNotFoundError(f\"Could not find meta-classifier at: {meta_classifier_path}\")\n",
    "        \n",
    "        meta_model = joblib.load(meta_classifier_path)\n",
    "        return meta_model\n",
    "\n",
    "    def _create_model_and_transform(self, folder_name: str, weight_path: str):\n",
    "        \"\"\"\n",
    "        1) Parse the folder name to identify the base model (e.g., 'EfficientNetB4', 'Resnext50', etc.).\n",
    "        2) Create and load that model, along with a transform that resizes to the required input dimension.\n",
    "        3) Return a dict containing the model and the transform.\n",
    "        \"\"\"\n",
    "        base_model_name = self._parse_base_model_name(folder_name)\n",
    "        if base_model_name not in imageSizePerCNN:\n",
    "            raise ValueError(f\"Unrecognized base model in '{folder_name}'. \"\n",
    "                             f\"Expected one of {list(imageSizePerCNN.keys())}, \"\n",
    "                             f\"but got '{base_model_name}'.\")\n",
    "\n",
    "        # Get required image size\n",
    "        image_size = imageSizePerCNN[base_model_name]\n",
    "\n",
    "        # Define the transform for this specific model\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Create and load the model\n",
    "        model = self._load_model_architecture(base_model_name)\n",
    "        state_dict = torch.load(weight_path, map_location=self.device)\n",
    "        model = loadModelCheckpoint(weight_path, model)\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        return model, transform\n",
    "\n",
    "    def _parse_base_model_name(self, folder_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Given a subfolder name like 'EfficientNetB4_0Fold' or 'Resnext_4Fold',\n",
    "        extract the correct key from 'imageSizePerCNN'.\n",
    "        We'll do a simple series of 'if' checks for known possibilities.\n",
    "        Adjust as needed if your naming is more complex.\n",
    "        \"\"\"\n",
    "        folder_name_lower = folder_name.lower()\n",
    "\n",
    "        if 'efficientnetb7' in folder_name_lower:\n",
    "            return 'EfficientNetB7'\n",
    "        elif 'efficientnetb6' in folder_name_lower:\n",
    "            return 'EfficientNetB6'\n",
    "        elif 'efficientnetb5' in folder_name_lower:\n",
    "            return 'EfficientNetB5'\n",
    "        elif 'efficientnetb4' in folder_name_lower:\n",
    "            return 'EfficientNetB4'\n",
    "        elif 'resnext' in folder_name_lower:\n",
    "            # For 'Resnext_4Fold', let's assume it means 'Resnext50'\n",
    "            return 'Resnext50'\n",
    "        elif 'resnet152' in folder_name_lower:\n",
    "            return 'Resnet152'\n",
    "        else:\n",
    "            # If none matched, you can handle it or raise an error\n",
    "            raise ValueError(f\"Could not parse a known model name from: {folder_name}\")\n",
    "\n",
    "    def _load_model_architecture(self, base_model_name: str):\n",
    "        \"\"\"\n",
    "        Create a model for the given base_model_name using torch.hub or efficientnet_pytorch.\n",
    "        We assume 8 output classes. Modify as needed.\n",
    "        \"\"\"\n",
    "        if base_model_name == \"Resnext50\":\n",
    "            model = torch.hub.load(\n",
    "                'pytorch/vision:v0.9.0',\n",
    "                'resnext50_32x4d',\n",
    "                pretrained=True\n",
    "            )\n",
    "            num_f = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_f, 8)\n",
    "\n",
    "        elif base_model_name == \"Resnet152\":\n",
    "            model = torch.hub.load(\n",
    "                'pytorch/vision:v0.9.0',\n",
    "                'resnet152',\n",
    "                pretrained=True\n",
    "            )\n",
    "            num_f = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_f, 8)\n",
    "\n",
    "        elif base_model_name == \"EfficientNetB6\":\n",
    "            # from efficientnet_pytorch import EfficientNet\n",
    "            model = EfficientNet.from_pretrained('efficientnet-b6', num_classes=8)\n",
    "\n",
    "        elif base_model_name == \"EfficientNetB7\":\n",
    "            model = EfficientNet.from_pretrained('efficientnet-b7', num_classes=8)\n",
    "\n",
    "        elif base_model_name == \"EfficientNetB5\":\n",
    "            model = EfficientNet.from_pretrained('efficientnet-b5', num_classes=8)\n",
    "\n",
    "        elif base_model_name == \"EfficientNetB4\":\n",
    "            model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=8)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"No architecture defined for {base_model_name}.\")\n",
    "\n",
    "        model = PLModel('base_model_name', model)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, image_path: str):\n",
    "        \"\"\"\n",
    "        Runs inference for each subfolder model, but then aggregates (averages) *logits* by architecture.\n",
    "        \n",
    "        Returns a dictionary keyed by architecture name, each containing:\n",
    "        {\n",
    "           \"aggregated_logits\": [8 floats],\n",
    "           \"aggregated_probabilities\": [8 floats],\n",
    "           \"predicted_class\": str\n",
    "        }\n",
    "        \"\"\"\n",
    "        # 1) Group subfolders by architecture\n",
    "        arch_to_subfolders = {}\n",
    "        for subfolder_name, model_obj in self.subfolder_models.items():\n",
    "            arch_to_subfolders.setdefault(model_obj[\"name\"], []).append(subfolder_name)\n",
    "\n",
    "        # 2) Open the image once\n",
    "        original_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # We'll store final predictions in a dict keyed by architecture\n",
    "        final_predictions = {}\n",
    "\n",
    "        # 3) For each architecture, gather & average the logits from all folds\n",
    "        for arch_name, subfolders in arch_to_subfolders.items():\n",
    "            logits_list = []\n",
    "\n",
    "            # For each fold subfolder, run inference & store logits\n",
    "            for sf in subfolders:\n",
    "                model = self.subfolder_models[sf][\"model\"]\n",
    "                transform = self.subfolder_models[sf][\"transform\"]\n",
    "\n",
    "                # Transform the image, run model\n",
    "                input_tensor = transform(original_image).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)   # shape [1, 8]\n",
    "                # Convert to CPU numpy (shape [8,])\n",
    "                logits = output.squeeze(0).cpu().numpy()\n",
    "                logits_list.append(logits)\n",
    "\n",
    "            # Average the logits across all folds\n",
    "            avg_logits = sum(logits_list) / len(logits_list)\n",
    "\n",
    "            # Convert to torch tensor for convenience\n",
    "            avg_logits_t = torch.tensor(avg_logits, dtype=torch.float32).unsqueeze(0)  # shape [1,8]\n",
    "            # Probability from averaged logits\n",
    "            avg_probs_t = torch.softmax(avg_logits_t, dim=1)\n",
    "            avg_probs = avg_probs_t.squeeze(0).tolist()\n",
    "\n",
    "            # Predicted class from the averaged logits\n",
    "            pred_idx = torch.argmax(avg_logits_t, dim=1).item()\n",
    "            pred_label = CLASS_LABELS[pred_idx]\n",
    "\n",
    "            final_predictions[arch_name] = {\n",
    "                \"logits\": avg_logits.tolist(),\n",
    "                \"probabilities\": avg_probs,\n",
    "                \"class\": pred_label\n",
    "            }\n",
    "\n",
    "        return final_predictions\n",
    "\n",
    "    def predict_meta(self, image_path: str, age: float, anatomic_site: str, sex: str):\n",
    "        \"\"\"\n",
    "        1) Averages CNN logits by architecture (via `predict(image_path)`).\n",
    "        2) Orders them in [EfficientNetB4, EfficientNetB5, EfficientNetB6, Resnext50, Resnet152].\n",
    "        3) Appends age, anatomic_site, sex.\n",
    "        4) Feeds feature vector into meta-classifier for final prediction.\n",
    "        \"\"\"\n",
    "        # 1) Get aggregated predictions by architecture\n",
    "        arch_predictions = self.predict(test_image_path)\n",
    "                \n",
    "        # 2) Build feature vector in the specified architecture order\n",
    "        feature_vector = []\n",
    "        feature_name = []\n",
    "        for arch in ARCHITECTURE_ORDER:\n",
    "            if arch in arch_predictions:\n",
    "                # Use the averaged logits\n",
    "                avg_logits = arch_predictions[arch][\"logits\"]\n",
    "                # Append them in order\n",
    "                feature_vector.extend(avg_logits)\n",
    "                for class_name in CLASS_LABELS:\n",
    "                    feature_name.append(f\"{arch}_{class_name}\")\n",
    "                \n",
    "        \n",
    "        # 3) Append metadata        \n",
    "        feature_vector.append(age)\n",
    "        feature_name.append(\"age_approx\")\n",
    "        feature_vector.append(anatomic_site)\n",
    "        feature_name.append(\"anatom_site_general\")\n",
    "        feature_vector.append(sex)\n",
    "        feature_name.append(\"sex\")\n",
    "        \n",
    "        # 4) Reshape for sklearn\n",
    "        X = pd.DataFrame(\n",
    "            [feature_vector],\n",
    "            columns = feature_name\n",
    "        )\n",
    "        \n",
    "        # 5) Predict with meta-classifier\n",
    "        meta_pred = self.meta_model.predict(X)  # final class\n",
    "        meta_proba = None\n",
    "        if hasattr(self.meta_model, \"predict_proba\"):\n",
    "            meta_proba = self.meta_model.predict_proba(X)[0].tolist()\n",
    "\n",
    "        return {\n",
    "            \"meta_predicted_label\": meta_pred,\n",
    "            \"classes\" : self.meta_model.classes_,\n",
    "            \"meta_probabilities\": meta_proba\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cae03717-778d-485b-afd0-cf374fd4e240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cino/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "No loss specified, using default\n",
      "Model Loaded\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "No loss specified, using default\n",
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "classifier = SkinDiseaseClassifier(\n",
    "    base_folder=\"checkpoints\", \n",
    "    metaclassifier_name = \"rf_patient_data\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b36820cb-021e-431e-adcf-cdfc774d7831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EfficientNetB4': {'logits': [-3.090391159057617,\n",
       "   10.048822402954102,\n",
       "   -1.8956981897354126,\n",
       "   -6.83709716796875,\n",
       "   1.0210392475128174,\n",
       "   -5.339770793914795,\n",
       "   -0.6963459253311157,\n",
       "   -5.117497444152832],\n",
       "  'probabilities': [1.9662857084767893e-06,\n",
       "   0.9998493194580078,\n",
       "   6.493741693702759e-06,\n",
       "   4.63951792539774e-08,\n",
       "   0.00012001016875728965,\n",
       "   2.0737348904731334e-07,\n",
       "   2.154602225346025e-05,\n",
       "   2.589915197859227e-07],\n",
       "  'class': 'NV'},\n",
       " 'Resnext50': {'logits': [3.4767792224884033,\n",
       "   6.28027868270874,\n",
       "   -5.540708541870117,\n",
       "   -14.302490234375,\n",
       "   0.22887957096099854,\n",
       "   -11.62803840637207,\n",
       "   -6.831872463226318,\n",
       "   -10.786775588989258],\n",
       "  'probabilities': [0.057008299976587296,\n",
       "   0.940767765045166,\n",
       "   6.913416200404754e-06,\n",
       "   1.082677836272694e-09,\n",
       "   0.002215098822489381,\n",
       "   1.5703587763482574e-08,\n",
       "   1.9008487015526043e-06,\n",
       "   3.6421241134121374e-08],\n",
       "  'class': 'NV'},\n",
       " 'Resnet152': {'logits': [1.9155490398406982,\n",
       "   4.948451042175293,\n",
       "   -2.397677183151245,\n",
       "   -16.979406356811523,\n",
       "   -1.2609962224960327,\n",
       "   -11.947687149047852,\n",
       "   -7.51156759262085,\n",
       "   -10.873723030090332],\n",
       "  'probabilities': [0.045845091342926025,\n",
       "   0.9516241550445557,\n",
       "   0.0006138784810900688,\n",
       "   2.85310941539052e-10,\n",
       "   0.0019130957080051303,\n",
       "   4.370853901036753e-08,\n",
       "   3.6910362268827157e-06,\n",
       "   1.279329921999306e-07],\n",
       "  'class': 'NV'},\n",
       " 'EfficientNetB6': {'logits': [1.2500313520431519,\n",
       "   9.140898704528809,\n",
       "   -1.7913262844085693,\n",
       "   -5.027579307556152,\n",
       "   -0.23962953686714172,\n",
       "   -5.499424457550049,\n",
       "   -1.8217443227767944,\n",
       "   -5.366804122924805],\n",
       "  'probabilities': [0.00037395968683995306,\n",
       "   0.999504804611206,\n",
       "   1.7864043911686167e-05,\n",
       "   7.022521231192513e-07,\n",
       "   8.430884190602228e-05,\n",
       "   4.3809987460008415e-07,\n",
       "   1.7328846297459677e-05,\n",
       "   5.002298166800756e-07],\n",
       "  'class': 'NV'},\n",
       " 'EfficientNetB5': {'logits': [0.7417600750923157,\n",
       "   9.882835388183594,\n",
       "   -3.96728515625,\n",
       "   -6.168178081512451,\n",
       "   -0.5822089910507202,\n",
       "   -4.873664855957031,\n",
       "   -1.5917181968688965,\n",
       "   -4.332239151000977],\n",
       "  'probabilities': [0.0001071561491698958,\n",
       "   0.9998517036437988,\n",
       "   9.65838808042463e-07,\n",
       "   1.0692242824461573e-07,\n",
       "   2.8511805794551037e-05,\n",
       "   3.901835725628189e-07,\n",
       "   1.038963000610238e-05,\n",
       "   6.705129180772929e-07],\n",
       "  'class': 'NV'}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier.predict(\"imagesToTest/IMG_3906.jpg\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "762f25f2-64f0-4f24-89b0-79cfabc9d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Classifier Output:\n",
      "  Predicted Label: ['NV']\n",
      "  Classes: ['AK' 'BCC' 'BKL' 'DF' 'MEL' 'NV' 'SCC' 'VASC']\n",
      "  Probabilities: [0.000788882653673682, 0.0007411998785662256, 0.02709074486066897, 0.0, 0.0699102664292883, 0.899293978702336, 0.0021749274754668843, 0.0]\n"
     ]
    }
   ],
   "source": [
    "test_image_path = \"imagesToTest/IMG_3906.jpg\"\n",
    "age = 30\n",
    "\n",
    "#head/neck - Lesions located on the face, scalp, neck, ears, etc.\n",
    "#upper extremity - Lesions found on arms, hands, and shoulders.\n",
    "#lower extremity - Lesions found on legs, feet, and hips.\n",
    "#torso - Lesions found on the chest, abdomen, back, etc.\n",
    "#palms/soles - Lesions located on the palms of the hands or soles of the feet.\n",
    "#oral/genital - Lesions located in mucosal regions, such as the mouth or genital areas.\n",
    "\n",
    "anatomic_site = \"torso\"\n",
    "sex = \"male\"\n",
    "\n",
    "# Final meta-classifier prediction\n",
    "meta_result = classifier.predict_meta(\n",
    "    image_path=test_image_path,\n",
    "    age=age,\n",
    "    anatomic_site=anatomic_site,\n",
    "    sex=sex\n",
    ")\n",
    "\n",
    "print(\"Meta-Classifier Output:\")\n",
    "print(\"  Predicted Label:\", meta_result[\"meta_predicted_label\"])\n",
    "print(\"  Classes:\", meta_result[\"classes\"])\n",
    "print(\"  Probabilities:\", meta_result[\"meta_probabilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd490436-3d3a-4e0c-bbf9-3986ae4ef62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
